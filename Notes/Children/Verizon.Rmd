

Verizon Repair Times
======================


Verizon is the primary local telephone company (incumbent local exchange carrier ILEC) for a large area of the eastern United States.  AS such, it is responsible for providing repair service for the customers of other telephone companies known as competing local exchange carriers (CLECs) in this region.  Verizon is subject to fines if the repair times (the time it takes to fix a problem) for CLEC customers are substantially worse than those for Verizon customers.  The data set **Verizon.csv** stored in the class **Data** directory contains a random sample of repair times for 1664 ILEC and 23 CLEC customers.  The mean repair time for ILEC customers is 8.4 hours, while that for CLEC customers is 16.5 hours.  Could a difference this large be easily explained by chance?

The permutation distribution (difference of means) is skewed to the left, but that does not matter; both the observed statistic and the permutation resamples are affected by imbalance and skewness in the same way.  This test works fine even with unbalanced sample sizes of 1664 and 23, and even for very skewed data.

Note that the Verizon data has long tails.  The mean is not the best measure of center to use with long tailed distributions.  We may want to use a statistic that is less sensitive to skewed distributions.  There are a number of reasons to do this.  One is to get a better measure of what is important in practice, how inconvenienced customers are by the repairs.  After a while, each additional hour probably does not matter as much, yet a sample mean treats an extra 10 hours on a repair time of 100 hours the same as an extra 10 hours on a repair of 1 hour.  Second, a large recorded repair time might just be a blunder; for example, a repair time of $10^6$ hours must be a mistake.  Third, a more robust statistic could be more sensitive at detecting real differences in the distributions -- the mean is so sensitive to large observations that it pays less attention to moderate observations, whereas a statistic more sensitive to moderate observations could detect differences between populations that show up in the moderate observations.

Using Means
=============

```{r VER, comment = NA}
Ver <- read.csv("http://www1.appstate.edu/~arnholta/Data/Verizon.csv")
head(Ver)
ANS <- tapply(Ver$Time, Ver$Group, mean)
observed <- ANS[1] - ANS[2]
observed
N <- 10^4 - 1         # number of times fo repeat the process
result <- numeric(N)  # space to save the random differences
set.seed(10)
for (i in 1:N){
  # sample of size 5, from 1 to 10, without replacement
  index <- sample(1687, size = 23, replace = FALSE)
  result[i] <- mean(Ver$Time[index]) - mean(Ver$Time[-index])
}
hist(result, col = "blue", main = "", breaks = "Scott")
abline(v = observed)
pvalue <- (sum(result >= observed) + 1)/(N + 1) # p-value
pvalue
DF <- data.frame(x = result)
p <- ggplot(data = DF) + 
  geom_density(aes(x = x, y = ..density..), fill = 'pink', alpha = 0.4) + 
  theme_bw()
p
x.dens <- density(result)
df.dens <- data.frame(x = x.dens$x, y = x.dens$y)
p + geom_area(data = subset(df.dens, x >= observed & x <= max(DF$x)), aes(x = x, y = y), fill = 'blue', alpha = 0.4) + labs(x = expression(bar(x)[CLEC] - bar(x)[ILEC]), y = '', title = "Permutation Distribution") 
```

Using Medians
=================


```{r VER2, comment = NA}
ANS <- tapply(Ver$Time, Ver$Group, median)
observed <- ANS[1] - ANS[2]
observed
N <- 10^4 - 1         # number of times fo repeat the process
result <- numeric(N)  # space to save the random differences
set.seed(9)
for (i in 1:N){
  # sample of size 5, from 1 to 10, without replacement
  index <- sample(1687, size = 23, replace = FALSE)
  result[i] <- median(Ver$Time[index]) - median(Ver$Time[-index])
}
hist(result, col = "blue", main = "Difference in Medians", breaks = "Scott")
abline(v = observed)
pvalue <- (sum(result >= observed) + 1)/(N + 1) # p-value
pvalue
DF <- data.frame(x = result)
p <- ggplot(data = DF) + 
  geom_density(aes(x = x, y = ..density..), fill = 'pink', alpha = 0.4) + 
  theme_bw()
p
x.dens <- density(result)
df.dens <- data.frame(x = x.dens$x, y = x.dens$y)
p + geom_area(data = subset(df.dens, x >= observed & x <= max(DF$x)), aes(x = x, y = y), fill = 'blue', alpha = 0.4) + labs(x = '', y = '') + 
  geom_vline(xintercept = observed, lty = "dashed") +
  labs(x = expression(tilde(x)[CLEC] - tilde(x)[ILEC]), y = '', title = "Permutation Distribution") 
```

Using 25% Trimmed Means
=================


```{r TM1, comment = NA}
ANS <- tapply(Ver$Time, Ver$Group, mean, trim = 0.25)
observed <- ANS[1] - ANS[2]
observed
N <- 10^4 - 1         # number of times fo repeat the process
result <- numeric(N)  # space to save the random differences
set.seed(8)
for (i in 1:N){
  # sample of size 5, from 1 to 10, without replacement
  index <- sample(1687, size = 23, replace = FALSE)
  result[i] <- mean(Ver$Time[index], trim = 0.25) - mean(Ver$Time[-index], trim = 0.25)
}
hist(result, col = "blue", main = "Difference in Trimmed Means", breaks = "Scott")
abline(v = observed, lty = "dashed")
pvalue1 <- (sum(result >= observed) + 1)/(N + 1) # p-value
pvalue1
DF <- data.frame(x = result)
p <- ggplot(data = DF) + 
  geom_density(aes(x = x, y = ..density..), fill = 'pink', alpha = 0.4) + 
  theme_bw()
p
x.dens <- density(result)
df.dens <- data.frame(x = x.dens$x, y = x.dens$y)
p + geom_area(data = subset(df.dens, x >= observed & x <= max(DF$x)), aes(x = x, y = y), fill = 'blue', alpha = 0.4) + 
  labs(x = expression(bar(x)[T.25.CLEC] - bar(x)[T.25.ILEC]), y = '', title = "Permutation Distribution") + 
  geom_vline(xintercept = observed, color = 'red', lty = "dashed")
```

It seems that the more robust statistics (median, 25% trimmed mean) are more sensitive to a possible difference between the populations; the tests are significant with estimated _p_-values of `r pvalue` and `r pvalue1`, respectively.